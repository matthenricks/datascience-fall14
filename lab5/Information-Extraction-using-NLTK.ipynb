{
 "metadata": {
  "name": "",
  "signature": "sha256:356587b89c52791770a3dc4d38089761a025018f48d9a09fa83c9d4285da79c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Part 2: Information Extraction using NLTK\n",
      "\n",
      "[NLTK](http://www.nltk.org/) is a python platform for natural language processing and information extraction. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, and an active discussion forum.\n",
      "\n",
      "The associated book, [Natural Language Processing with Python](http://www.nltk.org/book/) is also freely available and is a great resource both for NLP concepts and for practical examples of how to use the NLTK package.\n",
      "\n",
      "To get started, you need to install NLTK:\n",
      "    \n",
      "    sudo pip install -U nltk\n",
      "    \n",
      "After running the first command below (download()), you will be presented with a window to select which 'data' to download. For space reasons, only download 'book' (Everything used in the NLTK book). This in itself is a few hundred MB download."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "#nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Test it is working\n",
      "\n",
      "sentence = \"\"\"At eight o'clock on Thursday morning, Arthur didn't feel very good.\"\"\"\n",
      "tokens = nltk.word_tokenize(sentence)\n",
      "tagged = nltk.pos_tag(tokens)\n",
      "tagged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[('At', 'IN'),\n",
        " ('eight', 'CD'),\n",
        " (\"o'clock\", 'JJ'),\n",
        " ('on', 'IN'),\n",
        " ('Thursday', 'NNP'),\n",
        " ('morning', 'NN'),\n",
        " (',', ','),\n",
        " ('Arthur', 'NNP'),\n",
        " ('did', 'VBD'),\n",
        " (\"n't\", 'RB'),\n",
        " ('feel', 'VB'),\n",
        " ('very', 'RB'),\n",
        " ('good', 'JJ'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The above two commands tokenize the string, and tag each of the token with the part-of-speech. Here is a listing of tagsets that NLTK uses here (there are different tagsets used by different corpora).\n",
      "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following set of commands extracts named entities."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entities = nltk.chunk.ne_chunk(tagged)\n",
      "entities"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'JJ'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), (',', ','), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Visual representation of the tree \n",
      "entities.draw()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Named Entity Recognition \n",
      "\n",
      "As a somewhat more elaborate example, the following sequence of commands reads data from a file, and does NER on each of the sentences in the file. It doesn't do a very good job on this article, but in general, it seems to work quite well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with open(\"news1.html\", \"r\") as myfile:\n",
      "    data = myfile.read()   \n",
      "sentences = nltk.sent_tokenize(data)\n",
      "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
      "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
      "print(nltk.ne_chunk(sentences[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  CDC/NNP\n",
        "  considers/NNS\n",
        "  adding/VBG\n",
        "  names/NNS\n",
        "  of/IN\n",
        "  health/NN\n",
        "  workers/NNS\n",
        "  monitored/VBD\n",
        "  for/IN\n",
        "  (PERSON Ebola/NNP)\n",
        "  to/TO\n",
        "  no-fly/NNP\n",
        "  list/NN\n",
        "  (PERSON Published/NNP)\n",
        "  October/NNP\n",
        "  16/CD\n",
        "  ,/,\n",
        "  2014/CD\n",
        "  FoxNews.com/JJ\n",
        "  </NN\n",
        "  http/NN\n",
        "  :/:\n",
        "  //www.foxnews.com//JJ\n",
        "  >/NN\n",
        "  (PERSON Facebook/NNP)\n",
        "  </NNP\n",
        "  #/#\n",
        "  >/:\n",
        "  558/CD\n",
        "  Twitter/NNP\n",
        "  </NNP\n",
        "  #/#\n",
        "  >/:\n",
        "  597/CD\n",
        "  livefyre/NN\n",
        "  </:\n",
        "  #/#\n",
        "  >/:\n",
        "  1623/CD\n",
        "  Email/NNP\n",
        "  </NNP\n",
        "  #/#\n",
        "  >/:\n",
        "  Print/NNP\n",
        "  </:\n",
        "  #/#\n",
        "  >/:\n",
        "  Now/NNP\n",
        "  Playing/NNP\n",
        "  CDC/NNP\n",
        "  :/:\n",
        "  Second/NNP\n",
        "  (PERSON Dallas/NNP Ebola/NNP)\n",
        "  patient/NN\n",
        "  took/VBD\n",
        "  commercial/JJ\n",
        "  flight/NN\n",
        "  Never/RB\n",
        "  autoplay/NN\n",
        "  videos/NNS\n",
        "  </:\n",
        "  #/#\n",
        "  >/:\n",
        "  The/DT\n",
        "  (ORGANIZATION Centers/NNPS)\n",
        "  for/IN\n",
        "  (PERSON Disease/NNP Control/NNP)\n",
        "  and/CC\n",
        "  Prevention/NNP\n",
        "  is/VBZ\n",
        "  considering/VBG\n",
        "  adding/VBG\n",
        "  the/DT\n",
        "  names/NNS\n",
        "  of/IN\n",
        "  healthcare/NN\n",
        "  workers/NNS\n",
        "  being/VBG\n",
        "  monitored/VBN\n",
        "  for/IN\n",
        "  the/DT\n",
        "  (ORGANIZATION Ebola/NNP)\n",
        "  virus/VBZ\n",
        "  to/TO\n",
        "  the/DT\n",
        "  government/NN\n",
        "  's/POS\n",
        "  no-fly/JJ\n",
        "  list/NN\n",
        "  ,/,\n",
        "  federal/JJ\n",
        "  officials/NNS\n",
        "  tell/VBP\n",
        "  (PERSON Fox/NNP News/NNP)\n",
        "  ./.)\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Relation Extraction\n",
      "\n",
      "The second key task is to extract relations between entities. The following code snippet finds the relations between organizations and locations, in one of the existing datasets in NLTK. See the book webpage for more details on the regular expression pattern below: http://www.nltk.org/book/ch07.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "IN = re.compile(r'.*\\bin\\b(?!\\b.+ing)')\n",
      "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "    for rel in nltk.sem.extract_rels('ORG', 'LOC', doc, corpus='ieer', pattern = IN):\n",
      "        print(nltk.sem.rtuple(rel))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ORG: u'WHYY'] u'in' [LOC: u'Philadelphia']\n",
        "[ORG: u'McGlashan &AMP; Sarrail'] u'firm in' [LOC: u'San Mateo']\n",
        "[ORG: u'Freedom Forum'] u'in' [LOC: u'Arlington']\n",
        "[ORG: u'Brookings Institution'] u', the research group in' [LOC: u'Washington']\n",
        "[ORG: u'Idealab'] u', a self-described business incubator based in' [LOC: u'Los Angeles']\n",
        "[ORG: u'Open Text'] u', based in' [LOC: u'Waterloo']\n",
        "[ORG: u'WGBH'] u'in' [LOC: u'Boston']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ORG: u'Bastille Opera'] u'in' [LOC: u'Paris']\n",
        "[ORG: u'Omnicom'] u'in' [LOC: u'New York']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[ORG: u'DDB Needham'] u'in' [LOC: u'New York']\n",
        "[ORG: u'Kaplan Thaler Group'] u'in' [LOC: u'New York']\n",
        "[ORG: u'BBDO South'] u'in' [LOC: u'Atlanta']\n",
        "[ORG: u'Georgia-Pacific'] u'in' [LOC: u'Atlanta']\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment \n",
      "\n",
      "**PART 2.1 (NER):** Download, say 10 recent news articles, on some topic. Write code to extract named entities from each of them. The final output should simply be a list of entities and their types, which would require understanding the structure of the output of the ne_chunk command, and traversing it to find just the named entities. \n",
      "\n",
      "Submit both the python code, and the entities you extracted. For example, for the article above, the output should be:\n",
      "\n",
      "    Ebola, PERSON\n",
      "    Published, PERSON\n",
      "    Facebook, PERSON\n",
      "    ...\n",
      "\n",
      "**Part 2.2 (Relation Extraction):** Write a few regular expressions to extract different types of PERSON-ORGANIZATION relationships (e.g., PERSON executive at ORGANIZATION) over the same dataset (the IEER Corpus). You can use the above script mostly unchanged with the changes being: definition of the pattern IN, and the arguments to `extract_rels`. \n",
      "\n",
      "It may be useful to see the text of some of the documents, e.g., the second document in the above corpus can be seen by running:\n",
      "\n",
      "    print(nltk.corpus.ieer.parsed_docs('NYT_19980315')[1].text)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "imported_stories = [\n",
      "                    'http://www.nytimes.com/reuters/2014/09/30/business/30reuters-financial-markets-issues.html',\n",
      "                    'http://www.nytimes.com/reuters/2014/10/03/business/03reuters-alibaba-group-stocks.html',\n",
      "                    'http://www.nytimes.com/reuters/2014/10/09/business/09reuters-peel-funding.html',\n",
      "                    'http://www.nytimes.com/reuters/2014/10/16/business/16reuters-china-alibaba.html',\n",
      "                    'http://www.nytimes.com/aponline/2014/10/21/business/ap-us-earns-yahoo.html',\n",
      "                    'http://www.nytimes.com/2014/10/21/business/international/to-limit-risks-as-it-enters-china-costco-works-with-alibaba.html',\n",
      "                    'http://bits.blogs.nytimes.com/2014/10/22/daily-report-yahoo-growing-again-but-strategy-remains-hazy/',\n",
      "                    'http://www.nytimes.com/reuters/2014/10/22/arts/22reuters-alibabagroup-onlinecontent-china.html?_r=0',\n",
      "                    'http://www.cnbc.com/id/102020026',\n",
      "                    'http://www.alizila.com/alibaba-group%E2%80%99s-china-only-wholesale-website-going-abroad'\n",
      "                    ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up Goose to import in article information\n",
      "from goose import Goose\n",
      "import urllib2\n",
      "import re\n",
      "\n",
      "\n",
      "opener = urllib2.build_opener(urllib2.HTTPCookieProcessor())\n",
      "count = 1\n",
      "for url in imported_stories:\n",
      "    print 'Article ' + str(count)\n",
      "    count += 1\n",
      "    \n",
      "    response = opener.open(url)\n",
      "    raw_html = response.read()\n",
      "    g = Goose()\n",
      "    a = g.extract(raw_html=raw_html)\n",
      "\n",
      "    text = a.cleaned_text\n",
      "    \n",
      "    # Set up the NLTK parser to be able to extract named entities from\n",
      "    sentences = nltk.tokenize.sent_tokenize(text)\n",
      "    tokens = [nltk.tokenize.word_tokenize(y) for y in sentences]\n",
      "    pos_tags = [nltk.pos_tag(y) for y in tokens]\n",
      "    chunks = [nltk.ne_chunk(y, binary=False) for y in pos_tags]\n",
      "    \n",
      "    IN = re.compile(r'.*(PERSON|ORGANIZATION|LOCATION|DATE|TIME|MONEY|PERCENT|FACILITY|GPE).*')\n",
      "    \n",
      "    for sent in chunks:\n",
      "        for val in sent:\n",
      "            if isinstance(val, nltk.tree.Tree):\n",
      "                mat = IN.match(val.label())\n",
      "                if (mat != None):\n",
      "                    output = \"\"\n",
      "                    for leaf in val:\n",
      "                        value, part_of_speech = leaf\n",
      "                        output += ' ' + value\n",
      "                    print output + ', ' + val.label()\n",
      "    print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Article 1\n",
        " KONG, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Alibaba, GPE\n",
        " Worldwide, GPE\n",
        " ECM, ORGANIZATION\n",
        " Thomson Reuters, PERSON\n",
        " European, GPE\n",
        " IPOs, ORGANIZATION\n",
        " Jack Ma, PERSON\n",
        " Jet Li, PERSON\n",
        " New York, GPE\n",
        " Asia, GPE\n",
        " Equity, GPE\n",
        " China, GPE\n",
        " Hong Kong, GPE\n",
        " Australia, GPE\n",
        " ECM, ORGANIZATION\n",
        " Alibaba, PERSON\n",
        " Goldman, PERSON\n",
        " Sachs, PERSON\n",
        " ECM, ORGANIZATION\n",
        " JP Morgan, ORGANIZATION\n",
        " Morgan Stanley, PERSON\n",
        " Mervyn Chow, PERSON\n",
        " Asia, GPE\n",
        " Credit Suisse, ORGANIZATION\n",
        " Alibaba, PERSON\n",
        " Chinese, GPE\n",
        " Issuance, GPE\n",
        " Hong Kong, GPE\n",
        " Thailand, GPE\n",
        " Australia, GPE\n",
        " Key, GPE\n",
        " China, GPE\n",
        " Wanda Commercial, PERSON\n",
        " Australian, GPE\n",
        " Medibank Private, ORGANIZATION\n",
        " Australian, GPE\n",
        " Share, GPE\n",
        " Credit Suisse, ORGANIZATION\n",
        " Chow, ORGANIZATION\n",
        " European, GPE\n",
        " Nick Williams, PERSON\n",
        " Williams, PERSON\n",
        " European, GPE\n",
        " ECM, ORGANIZATION\n",
        " Credit Suisse, ORGANIZATION\n",
        " European, GPE\n",
        " ECM, ORGANIZATION\n",
        " German, GPE\n",
        " Bayer, PERSON\n",
        " Frankfurt, GPE\n",
        " Rocket Internet, PERSON\n",
        " Zalando, GPE\n",
        " Rocket, GPE\n",
        " United States, GPE\n",
        " Alibaba, PERSON\n",
        " Alibaba, PERSON\n",
        " Jackie Kelly, PERSON\n",
        " Americas IPO, PERSON\n",
        " Ernst, ORGANIZATION\n",
        " Young, PERSON\n",
        "\n",
        "Article 2\n",
        " Reuters, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Chinese, GPE\n",
        " Alibaba Group Holding Limited, ORGANIZATION\n",
        " Alibaba, PERSON\n",
        "\n",
        "Article 3\n",
        " Reuters, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Peel, GPE\n",
        " TVs, ORGANIZATION\n",
        " Chinese, GPE\n",
        " Alibaba Group Holding Ltd, ORGANIZATION\n",
        " Peel Smart Remote, ORGANIZATION\n",
        " Android, PERSON\n",
        " iOS, ORGANIZATION\n",
        " TVs, ORGANIZATION\n",
        " PCs, ORGANIZATION\n",
        " Alibaba, GPE\n",
        " United States, GPE\n",
        " Peel, GPE\n",
        " Peel, GPE\n",
        " Peel, PERSON\n",
        " Arunachalam, PERSON\n",
        " Alibaba, GPE\n",
        " Peel, GPE\n",
        " Redpoint Ventures, PERSON\n",
        " Lightspeed Venture Partners, PERSON\n",
        " Harrison Metal, PERSON\n",
        " Translink Capital, PERSON\n",
        "\n",
        "Article 4\n",
        " BEIJING, GPE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Chinese, GPE\n",
        " Alibaba Group Holding Ltd, ORGANIZATION\n",
        " Ant Financial Services Group, ORGANIZATION\n",
        " Alibaba, GPE\n",
        " Alipay, GPE\n",
        " Chinese, GPE\n",
        " Alibaba, PERSON\n",
        " Alipay, PERSON\n",
        " China, GPE\n",
        " Alipay Wallet, PERSON\n",
        " Alipay, ORGANIZATION\n",
        " Zhejiang Ant Small, PERSON\n",
        " Micro Financial Services Group Co, PERSON\n",
        " Alibaba, PERSON\n",
        " Beijing, GPE\n",
        " Ant Financial, PERSON\n",
        " Alipay, GPE\n",
        " China, GPE\n",
        " Ant Financial, PERSON\n",
        " Lucy Peng, PERSON\n",
        " Alibaba, GPE\n",
        " Alipay, GPE\n",
        " Jack Ma, PERSON\n",
        " Alibaba, ORGANIZATION\n",
        " Chinese, GPE\n",
        " Alibaba, PERSON\n",
        " New York, GPE\n",
        " Alibaba, GPE\n",
        " Alipay, GPE\n",
        " Alipay Wallet, PERSON\n",
        " Bao, PERSON\n",
        " Zhao Cai Bao, PERSON\n",
        " Ant Micro, PERSON\n",
        " MYBank, ORGANIZATION\n",
        " MYBank, ORGANIZATION\n",
        " Chinese, GPE\n",
        " Alibaba, PERSON\n",
        " IPO, ORGANIZATION\n",
        " Alipay, GPE\n",
        " Alibaba, GPE\n",
        " Ant, PERSON\n",
        " Peng, PERSON\n",
        " Ant, ORGANIZATION\n",
        "\n",
        "Article 5\n",
        " SUNNYVALE, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Calif., GPE\n",
        " Yahoo, PERSON\n",
        " Alibaba, PERSON\n",
        " IPO, ORGANIZATION\n",
        " Revenue, GPE\n",
        " Yahoo, GPE\n",
        " CEO Marissa Mayer, ORGANIZATION\n",
        " Yahoo, GPE\n",
        " Sunnyvale, ORGANIZATION\n",
        " California, GPE\n",
        " Alibaba, PERSON\n",
        " Yahoo, PERSON\n",
        " Inc., ORGANIZATION\n",
        " Chinese, GPE\n",
        " IPO, ORGANIZATION\n",
        " Yahoo, GPE\n",
        " Alibaba, GPE\n",
        " Alibaba, ORGANIZATION\n",
        " Yahoo, PERSON\n",
        " FactSet, ORGANIZATION\n",
        " Yahoo, GPE\n",
        " Yahoo, PERSON\n",
        "\n",
        "Article 6\n",
        " CHICAGO, GPE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Chinese, GPE\n",
        " American, GPE\n",
        " Costco Wholesale, PERSON\n",
        " Walmart, PERSON\n",
        " China, GPE\n",
        " China, GPE\n",
        " Walmart, GPE\n",
        " Best Buy, PERSON\n",
        " eBay, ORGANIZATION\n",
        " Alibaba, ORGANIZATION\n",
        " Tmall, PERSON\n",
        " China, GPE\n",
        " Costco, PERSON\n",
        " Walmart, PERSON\n",
        " Home Depot, PERSON\n",
        " Anjee Solanki, PERSON\n",
        " Colliers International, ORGANIZATION\n",
        " China, GPE\n",
        " David Cheesewright, PERSON\n",
        " Walmart, FACILITY\n",
        " China, GPE\n",
        " Walmart, PERSON\n",
        " Chinese, GPE\n",
        " China, GPE\n",
        " Club, ORGANIZATION\n",
        " Low, PERSON\n",
        " Chinese, GPE\n",
        " Walmart, PERSON\n",
        " Walmart, PERSON\n",
        " Sun Art Retail Group, ORGANIZATION\n",
        " China, GPE\n",
        " Kantar Retail, PERSON\n",
        " Walmart, GPE\n",
        " Mr. Cheesewright, PERSON\n",
        " Walmart, ORGANIZATION\n",
        " Alibaba, PERSON\n",
        " July, GPE\n",
        " iResearch, ORGANIZATION\n",
        " Chinese, GPE\n",
        " Chinese, GPE\n",
        " Virtual, GPE\n",
        " Tmall, GPE\n",
        " China, GPE\n",
        " Tmall, GPE\n",
        " Marcie Merriman, PERSON\n",
        " Costco, GPE\n",
        " China, GPE\n",
        " Kirkland, GPE\n",
        " Chinese, GPE\n",
        " Yihaodian, GPE\n",
        " Walmart, PERSON\n",
        " Tmall, GPE\n",
        " Costco, PERSON\n",
        " Sensodyne, ORGANIZATION\n",
        " Yihaodian, GPE\n",
        " Alibaba, ORGANIZATION\n",
        " Ms. Solanki, PERSON\n",
        " Colliers, ORGANIZATION\n",
        " China, GPE\n",
        " Rob Howard, PERSON\n",
        " Grand Junction, PERSON\n",
        " San Francisco, GPE\n",
        " Alibaba, ORGANIZATION\n",
        " Costco, PERSON\n",
        " Alibaba, PERSON\n",
        "\n",
        "Article 7\n",
        " Marissa, PERSON"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Vindu Goel, PERSON\n",
        " Yahoo, PERSON\n",
        " Wall, PERSON\n",
        " Yahoo, GPE\n",
        " Yahoo, PERSON\n",
        " Alibaba Group, ORGANIZATION\n",
        " Yahoo Japan, ORGANIZATION\n",
        "\n",
        "Article 8\n",
        " Reuters, ORGANIZATION"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Chinese, GPE\n",
        " Alibaba Group Holding Ltd, ORGANIZATION\n",
        " United States, GPE\n",
        " Bloomberg, PERSON\n",
        " Alibaba, PERSON\n",
        " Group, ORGANIZATION\n",
        " Walt Disney Co, PERSON\n",
        " Viacom Inc, PERSON\n",
        " Paramount Pictures, ORGANIZATION\n",
        " Time Warner Inc, PERSON\n",
        " Warner Bros, PERSON\n",
        " Sony Corp, PERSON\n",
        " Comcast Corp, ORGANIZATION\n",
        " Universal, ORGANIZATION\n",
        " U.S., GPE\n",
        " China, GPE\n",
        " Alibaba, PERSON\n",
        " Jack Ma, PERSON\n",
        " Los Angeles, GPE\n",
        " WSJDLive, ORGANIZATION\n",
        " Reuters, ORGANIZATION\n",
        " Alibaba, PERSON\n",
        " Alibaba, PERSON\n",
        " Chinese, GPE\n",
        " Alibaba, PERSON\n",
        " July, GPE\n",
        " Alibaba, PERSON\n",
        " Lions Gate Entertainment Corp, PERSON\n",
        " China, GPE\n",
        " March, GPE\n",
        " Alibaba, PERSON\n",
        " ChinaVision Media Group, ORGANIZATION\n",
        " Chinese, GPE\n",
        " Youku Tudou Inc, PERSON\n",
        " Yunfeng Capital, PERSON\n",
        " Alibaba, GPE\n",
        " Yunfeng Capital, PERSON\n",
        " Alibaba, GPE\n",
        "\n",
        "Article 9\n",
        " Alibaba, GPE"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " Jack Ma, PERSON\n",
        " IPO, ORGANIZATION\n",
        " Alibaba, PERSON\n",
        " Group, ORGANIZATION\n",
        " Chinese, GPE\n",
        " Alibaba, GPE\n",
        " Reuters, ORGANIZATION\n",
        " Alibaba, GPE\n",
        " Agricultural Bank, ORGANIZATION\n",
        " China, GPE\n",
        " ICBC, ORGANIZATION\n",
        " Alibaba, GPE\n",
        " IPO, ORGANIZATION\n",
        " Alibaba, GPE\n",
        " American Depositary Shares, ORGANIZATION\n",
        " IPO, ORGANIZATION\n",
        " Alibaba, GPE\n",
        " Yahoo, PERSON\n",
        " Alibaba, PERSON\n",
        " Jack Ma, PERSON\n",
        " Joe Tsai, PERSON\n",
        " Nasdaq, ORGANIZATION\n",
        " Yahoo, PERSON\n",
        " Chinese, GPE\n",
        " United States, GPE\n",
        " Baidu, PERSON\n",
        " Weibo, PERSON\n",
        " Alibaba, PERSON\n",
        " New York, GPE\n",
        " Chinese, GPE\n",
        " Reuters, GPE\n",
        " CNBC, ORGANIZATION\n",
        " Bob Pisani, PERSON\n",
        "\n",
        "Article 10\n",
        " Alibaba, PERSON"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " China, GPE\n",
        " South Korean, GPE\n",
        " China, GPE\n",
        " PRC, ORGANIZATION\n",
        " Taobao Marketplace, PERSON\n",
        " Chinese, GPE\n",
        " August, GPE\n",
        " B2B, ORGANIZATION\n",
        " Direct From Overseas Markets, PERSON\n",
        " Chinese, GPE\n",
        " South Korea, GPE\n",
        " South, GPE\n",
        " Agriculture, GPE\n",
        " Food, PERSON\n",
        " Rural Affairs, ORGANIZATION\n",
        " South Korean, GPE\n",
        " Woongjin Foods, PERSON\n",
        " Daesang Corp., PERSON\n",
        " South Korean, LOCATION\n",
        " Korean, GPE\n",
        " Korean, GPE\n",
        " Chinese, GPE\n",
        " Chinese, GPE\n",
        " Yang Chao, PERSON\n",
        " China, GPE\n",
        " Chinese, GPE\n",
        " Overseas Markets, ORGANIZATION\n",
        " Southeast Asian, LOCATION\n",
        " Thailand, GPE\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "IN = re.compile(r'.*\\b.*\\s(?:executive|work|chief|for|at).*\\b(?!\\b.+ing)')\n",
      "for doc in nltk.corpus.ieer.parsed_docs('NYT_19980315'):\n",
      "    for rel in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='ieer', pattern = IN):\n",
      "        print(nltk.sem.rtuple(rel))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[PER: u'Mike Godwin'] u', chief counsel for the' [ORG: u'Electronic Frontier Foundation']\n",
        "[PER: u'Jack Balkin'] u\", director of the school's program. ``What happened at\" [ORG: u'Yale']\n",
        "[PER: u'William Gale'] u', an economist at the' [ORG: u'Brookings Institution']\n",
        "[PER: u'Joel Slemrod'] u', an economist at the' [ORG: u'University of Michigan']\n",
        "[PER: u'Alan Braverman'] u', Internet analyst at' [ORG: u'Credit Suisse First Boston']\n",
        "[PER: u'Abe Kleinfield'] u', a vice president at' [ORG: u'Open Text']\n",
        "[PER: u'Lorne Michaels'] u\", the executive producer of ``Saturday Night Live.''\" [ORG: u'TV Books']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[PER: u'Rick Yorn'] u', his manager at the firm' [ORG: u'Addis-Wechsler &AMP; Associates']\n",
        "[PER: u'Tom Rothman'] u', president of production at' [ORG: u'20th Century Fox']\n",
        "[PER: u'Charlotte Forest'] u', executive producer for' [ORG: u'Homestead Editorial']\n",
        "[PER: u'Richard Strauss'] u\"' ``Salome'' at\" [ORG: u'La Scala']\n",
        "[PER: u'John Wren'] u', the president and chief executive at' [ORG: u'Omnicom']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[PER: u'Linda Kaplan Thaler'] u', who worked at' [ORG: u'Wells']\n",
        "[PER: u'Kaplan Thaler'] u'had worked for' [ORG: u\"Toys ``R'' Us\"]\n",
        "[PER: u'Ken Haldin'] u', a spokesman for' [ORG: u'Georgia-Pacific']\n"
       ]
      }
     ],
     "prompt_number": 32
    }
   ],
   "metadata": {}
  }
 ]
}