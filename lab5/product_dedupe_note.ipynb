{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import os\n",
      "import csv\n",
      "import re\n",
      "import collections\n",
      "import logging\n",
      "import optparse\n",
      "from numpy import nan\n",
      "\n",
      "import dedupe\n",
      "from unidecode import unidecode"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "LDA Section"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Now, include the cleaning\n",
      "from gensim import corpora\n",
      "from gensim.models.ldamodel import LdaModel\n",
      "from nltk.stem.porter import *\n",
      "from nltk.corpus import stopwords\n",
      "from nltk.tokenize import WordPunctTokenizer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up the infrastructure to clean the text\n",
      "stopset = set(stopwords.words('english'))\n",
      "stemmer = PorterStemmer()\n",
      "def cleanText(column):\n",
      "    tokens = WordPunctTokenizer().tokenize(column)\n",
      "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 5]\n",
      "    final = [stemmer.stem(word) for word in clean]\n",
      "    myString = \"\"\n",
      "    for x in cleanText(column):\n",
      "        myString += x + ' '\n",
      "    return ([myString.split()])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.matutils import cossim\n",
      "\n",
      "# Dictionary Loading\n",
      "dictionary_path = 'dictionary.mm'\n",
      "corpus_path = 'corpus.mm'\n",
      "dictionary = corpora.Dictionary.load('dictionary.mm')\n",
      "lda_path = 'lda.mm'\n",
      "# model_tfidf = models.TfidfModel(mm_bow, id2word=id2word, normalize=True)\n",
      "\n",
      "if not os.path.isfile(lda_path):\n",
      "    corpus = corpora.MmCorpus('corpus.mm')\n",
      "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=100, passes=5)\n",
      "    lda_model.save(lda_path)\n",
      "\n",
      "lda_model.print_topics(20)\n",
      "\n",
      "def compare_descriptions(desc1, desc2):\n",
      "    dv_1 = dictionary.doc2bow(desc1.lower().split())\n",
      "    dv_2 = dictionary.doc2bow(desc2.lower().split())\n",
      "    \n",
      "    print dv_1\n",
      "    print dv_2\n",
      "    dv_1 = lda_model[dv_1]\n",
      "    dv_2 = lda_model[dv_2]\n",
      "    return cossim(dv_1, dv_2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "END LDA"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ## Logging - ADD THIS IN\n",
      "\n",
      "# Dedupe uses Python logging to show or suppress verbose output. Added for convenience.\n",
      "# To enable verbose logging, run `python examples/csv_example/csv_example.py -v`\n",
      "optp = optparse.OptionParser()\n",
      "optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
      "                help='Increase verbosity (specify multiple times for more)'\n",
      "                )\n",
      "(opts, args) = optp.parse_args()\n",
      "log_level = logging.WARNING \n",
      "if opts.verbose == 1:\n",
      "    log_level = logging.INFO\n",
      "elif opts.verbose >= 2:\n",
      "    log_level = logging.DEBUG\n",
      "logging.getLogger().setLevel(log_level)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "\"\\noptp = optparse.OptionParser()\\noptp.add_option('-v', '--verbose', dest='verbose', action='count',\\n                help='Increase verbosity (specify multiple times for more)'\\n                )\\n(opts, args) = optp.parse_args()\\nlog_level = logging.WARNING \\nif opts.verbose == 1:\\n    log_level = logging.INFO\\nelif opts.verbose >= 2:\\n    log_level = logging.DEBUG\\nlogging.getLogger().setLevel(log_level)\\n\""
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Switch to our working directory and set up our input and out put paths,\n",
      "# as well as our settings and training file locations\n",
      "input_file = 'products.csv'\n",
      "output_file = 'products_out.csv'\n",
      "settings_file = 'products_learned_settings'\n",
      "training_file = 'products_training.json'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def preProcess(column):\n",
      "    \"\"\"\n",
      "    Do a little bit of data cleaning with the help of Unidecode and Regex.\n",
      "    Things like casing, extra spaces, quotes and new lines can be ignored.\n",
      "    \"\"\"\n",
      "    column = unidecode(column)\n",
      "    column = re.sub('  +', ' ', column)\n",
      "    column = re.sub('\\n', ' ', column)\n",
      "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
      "    return column"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def readData(filename):\n",
      "    \"\"\"\n",
      "    Read in our data from a CSV file and create a dictionary of records, \n",
      "    where the key is a unique record ID and each value is dict\n",
      "    \"\"\"\n",
      "    data_d = {}\n",
      "    with open(filename) as f:\n",
      "        reader = csv.DictReader(f)\n",
      "        for row in reader:\n",
      "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
      "            row_id = row['id']\n",
      "            data_d[row_id] = dict(clean_row)\n",
      "\n",
      "    return data_d\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'importing data ...'\n",
      "data_d = readData(input_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "importing data ...\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:6: RuntimeWarning: Argument <type 'str'> is not an unicode object. Passing an encoded string will likely have unexpected results.\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# More advanced LDA comparator\n",
      "def descComparator(field_1, field_2):\n",
      "    return compare_descriptions(field_1, field_2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Price comparator\n",
      "gdp_usd = 1.61\n",
      "def convert_to_US(field):\n",
      "    if \"gbp\" in field:\n",
      "        nums = re.findall(r'\\b\\d+\\.*\\d*\\b', field)\n",
      "        return float(nums[0].strip()) * gdp_usd\n",
      "    else:\n",
      "        return float(field.strip())\n",
      "    \n",
      "def priceComparator(field_1, field_2) :\n",
      "\t# Recognize the currency\n",
      "    if field_1 and field_2 :\n",
      "        field_1 = convert_to_US(field_1)\n",
      "        field_2 = convert_to_US(field_2)\n",
      "        return (field_1 - field_2)\n",
      "    else :\n",
      "        return nan"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ## Training\n",
      "\n",
      "if os.path.exists(settings_file):\n",
      "    print 'reading from', settings_file\n",
      "    with open(settings_file, 'rb') as f:\n",
      "        deduper = dedupe.StaticDedupe(f)\n",
      "\n",
      "else:\n",
      "    # Here you will need to define the fields dedupe will pay attention to. You also need to define the comparator\n",
      "    # to be used and specify any customComparators. Please read the dedupe manual for details\n",
      "    fields = [\n",
      "        {'field' : 'title', 'type': 'String'},\n",
      "        {'field' : 'price', 'type': 'Custom', 'has missing':True, 'comparator' : priceComparator},\n",
      "        {'field' : 'manufacturer', 'type': 'String', 'has missing':True}\n",
      "        {'field' : 'description', 'type': 'Custom', 'has missing':True, 'comparator' : descComparator}\n",
      "        ]\n",
      "    \n",
      "    # Create a new deduper object and pass our data model to it.\n",
      "    deduper = dedupe.Dedupe(fields)\n",
      "\n",
      "    # To train dedupe, we feed it a random sample of records.\n",
      "    deduper.sample(data_d, 150000)\n",
      "\n",
      "\n",
      "    # If we have training data saved from a previous run of dedupe,\n",
      "    # look for it an load it in.\n",
      "    # __Note:__ if you want to train from scratch, delete the training_file\n",
      "    if os.path.exists(training_file):\n",
      "        print 'reading labeled examples from ', training_file\n",
      "        with open(training_file, 'rb') as f:\n",
      "            deduper.readTraining(f)\n",
      "\n",
      "    # ## Active learning\n",
      "    # Dedupe will find the next pair of records\n",
      "    # it is least certain about and ask you to label them as duplicates\n",
      "    # or not.\n",
      "    # use 'y', 'n' and 'u' keys to flag duplicates\n",
      "    # press 'f' when you are finished\n",
      "    print 'starting active labeling...'\n",
      "\n",
      "    dedupe.consoleLabel(deduper)\n",
      "\n",
      "    deduper.train()\n",
      "\n",
      "    # When finished, save our training away to disk\n",
      "    with open(training_file, 'w') as tf :\n",
      "        deduper.writeTraining(tf)\n",
      "\n",
      "    # Save our weights and predicates to disk.  If the settings file\n",
      "    # exists, we will skip all the training and learning next time we run\n",
      "    # this file.\n",
      "    with open(settings_file, 'w') as sf :\n",
      "        deduper.writeSettings(sf)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ## Blocking\n",
      "\n",
      "print 'blocking...'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# ## Clustering\n",
      "\n",
      "# Find the threshold that will maximize a weighted average of our precision and recall. \n",
      "# When we set the recall weight to 2, we are saying we care twice as much\n",
      "# about recall as we do precision.\n",
      "#\n",
      "# If we had more data, we would not pass in all the blocked data into\n",
      "# this function but a representative sample.\n",
      "\n",
      "threshold = deduper.threshold(data_d, recall_weight=2)\n",
      "\n",
      "# `match` will return sets of record IDs that dedupe\n",
      "# believes are all referring to the same entity.\n",
      "\n",
      "print 'clustering...'\n",
      "clustered_dupes = deduper.match(data_d, threshold)\n",
      "\n",
      "print '# duplicate sets', len(clustered_dupes)\n",
      "\n",
      "# ## Writing Results\n",
      "\n",
      "# Write our original data back out to a CSV with a new column called \n",
      "# 'Cluster ID' which indicates which records refer to each other.\n",
      "\n",
      "cluster_membership = {}\n",
      "cluster_id = 0\n",
      "for (cluster_id, cluster) in enumerate(clustered_dupes):\n",
      "    id_set, conf_score = cluster\n",
      "    cluster_d = [data_d[c] for c in id_set]\n",
      "    canonical_rep = dedupe.canonicalize(cluster_d)\n",
      "    for record_id in id_set:\n",
      "        cluster_membership[record_id] = {\n",
      "            \"cluster id\" : cluster_id,\n",
      "            \"canonical representation\" : canonical_rep,\n",
      "            \"confidence\": conf_score\n",
      "        }\n",
      "\n",
      "singleton_id = cluster_id + 1\n",
      "\n",
      "with open(output_file, 'w') as f_output:\n",
      "    writer = csv.writer(f_output)\n",
      "\n",
      "    with open(input_file) as f_input :\n",
      "        reader = csv.reader(f_input)\n",
      "\n",
      "        heading_row = reader.next()\n",
      "        heading_row.insert(0, 'Cluster ID')\n",
      "        canonical_keys = canonical_rep.keys()\n",
      "        for key in canonical_keys:\n",
      "            heading_row.append('canonical_' + key)\n",
      "        heading_row.append('confidence_score')\n",
      "        \n",
      "        writer.writerow(heading_row)\n",
      "\n",
      "        for row in reader:\n",
      "            row_id = row[1]\n",
      "            if row_id in cluster_membership:\n",
      "                cluster_id = cluster_membership[row_id][\"cluster id\"]\n",
      "                canonical_rep = cluster_membership[row_id][\"canonical representation\"]\n",
      "                row.insert(0, cluster_id)\n",
      "                for key in canonical_keys:\n",
      "                    row.append(canonical_rep[key])\n",
      "                row.append(cluster_membership[row_id]['confidence'])\n",
      "            else:\n",
      "                row.insert(0, singleton_id)\n",
      "                singleton_id += 1\n",
      "                for key in canonical_keys:\n",
      "                    row.append(None)\n",
      "                row.append(None)\n",
      "            writer.writerow(row)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}